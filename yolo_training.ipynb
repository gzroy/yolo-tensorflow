{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import contrib\n",
    "autograph = contrib.autograph\n",
    "import yolonet_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 448\n",
    "image_height = 448\n",
    "image_width_delta = 90   #448*1.2-448\n",
    "image_height_delta = 90  #448*1.2-448 \n",
    "batch_size = 64\n",
    "valid_batch_size = 1\n",
    "epoch_size = 26332\n",
    "grids=7\n",
    "lambda_noobj = 0.5\n",
    "lambda_obj = 5.0\n",
    "grid_width = image_width//grids\n",
    "grid_height = image_height//grids\n",
    "labels = ['person','bird','cat','cow','dog','horse','sheep','aeroplane','bicycle',\n",
    "          'boat','bus','car','motorbike','train','bottle','chair','diningtable',\n",
    "          'pottedplant','sofa','tvmonitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the function to parse the tfrecord**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    features = {\"image\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"height\": tf.FixedLenFeature([1], tf.int64, default_value=[0]),\n",
    "                \"width\": tf.FixedLenFeature([1], tf.int64, default_value=[0]),\n",
    "                \"channels\": tf.FixedLenFeature([1], tf.int64, default_value=[3]),\n",
    "                \"colorspace\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"img_format\": tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "                \"label\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_xmin\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_xmax\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_ymin\": tf.VarLenFeature(tf.int64),\n",
    "                \"bbox_ymax\": tf.VarLenFeature(tf.int64),\n",
    "                \"filename\": tf.FixedLenFeature([], tf.string, default_value=\"\")\n",
    "               }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    \n",
    "    label = tf.expand_dims(parsed_features[\"label\"].values, 0)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    height = parsed_features[\"height\"]\n",
    "    width = parsed_features[\"width\"]\n",
    "    channels = parsed_features[\"channels\"]\n",
    "\n",
    "    #Generate the random crop offset\n",
    "    random_width_start = tf.random.uniform([1], minval=0, maxval=image_width_delta, dtype=tf.dtypes.int64)\n",
    "    random_height_start = tf.random.uniform([1], minval=0, maxval=image_height_delta, dtype=tf.dtypes.int64)\n",
    "    random_start = tf.concat([random_height_start, random_width_start, tf.constant([0], dtype=tf.dtypes.int64)], axis=0)\n",
    "    \n",
    "    #Adjust the bbox coordinates with random crop offset\n",
    "    def f1():\n",
    "        xmin = tf.expand_dims(parsed_features[\"bbox_xmin\"].values, 0)\n",
    "        xmax = tf.expand_dims(parsed_features[\"bbox_xmax\"].values, 0)\n",
    "        ymin = tf.expand_dims(parsed_features[\"bbox_ymin\"].values, 0)\n",
    "        ymax = tf.expand_dims(parsed_features[\"bbox_ymax\"].values, 0)\n",
    "        xmin = xmin - random_width_start\n",
    "        xmin = tf.clip_by_value(xmin, 0, image_width)\n",
    "        xmax = xmax - random_width_start\n",
    "        xmax = tf.clip_by_value(xmax, 0, image_width)\n",
    "        ymin = ymin - random_height_start\n",
    "        ymin = tf.clip_by_value(ymin, 0, image_height)\n",
    "        ymax = ymax - random_height_start\n",
    "        ymax = tf.clip_by_value(ymax, 0, image_height)\n",
    "        return xmin, xmax, ymin, ymax\n",
    "    #Adjust the bbox coordinates with image flipped and random crop offset\n",
    "    def f2():\n",
    "        xmin = tf.expand_dims(parsed_features[\"bbox_xmin\"].values, 0)\n",
    "        xmax = tf.expand_dims(parsed_features[\"bbox_xmax\"].values, 0)\n",
    "        ymin = tf.expand_dims(parsed_features[\"bbox_ymin\"].values, 0)\n",
    "        ymax = tf.expand_dims(parsed_features[\"bbox_ymax\"].values, 0)\n",
    "        xmin_temp = xmin - random_width_start\n",
    "        xmax_temp = xmax - random_width_start\n",
    "        xmin = image_width - tf.clip_by_value(xmax_temp, 0, image_width)\n",
    "        xmax = image_width - tf.clip_by_value(xmin_temp, 0, image_width)\n",
    "        ymin = ymin - random_height_start\n",
    "        ymin = tf.clip_by_value(ymin, 0, image_height)\n",
    "        ymax = ymax - random_height_start\n",
    "        ymax = tf.clip_by_value(ymax, 0, image_height)\n",
    "        return xmin, xmax, ymin, ymax\n",
    "    \n",
    "    #Generate the random flip flag\n",
    "    random_flip = tf.random.uniform([1], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
    "    #Get the random flip and crop image coordinates\n",
    "    xmin, xmax, ymin, ymax = tf.cond(tf.less(random_flip[0], 0.5), f1, f2)\n",
    "    image_raw = tf.image.decode_jpeg(parsed_features[\"image\"], channels=3)\n",
    "    image_sliced = tf.slice(image_raw, random_start, [image_height, image_width, -1])\n",
    "    image_decoded = tf.image.convert_image_dtype(image_sliced, tf.float32)\n",
    "    image_flipped = tf.cond(tf.less(random_flip[0], 0.5), lambda:image_decoded, lambda:tf.image.flip_left_right(image_decoded))\n",
    "    image_train = tf.image.per_image_standardization(image_flipped)\n",
    "    \n",
    "    #Calculate the boxes center point\n",
    "    box_center_x = xmin+(xmax-xmin)//2\n",
    "    box_center_y = ymin+(ymax-ymin)//2\n",
    "    #Calculate the boxes relate to which grid\n",
    "    grid_id = (tf.ceil(box_center_y/grid_height)-1)*grids + tf.ceil(box_center_x/grid_width) - 1\n",
    "    grid_id = tf.cast(grid_id, tf.float32)\n",
    "    #Calculate and normalize the bbox center and width by grids\n",
    "    center_x_percent = box_center_x%grid_width/grid_width\n",
    "    center_x_percent = tf.cast(center_x_percent, tf.float32)\n",
    "    center_y_percent = box_center_y%grid_height/grid_height\n",
    "    center_y_percent = tf.cast(center_y_percent, tf.float32)\n",
    "    box_width = (xmax-xmin)/image_width\n",
    "    box_width = tf.cast(box_width, tf.float32)\n",
    "    box_height = (ymax-ymin)/image_height\n",
    "    box_height = tf.cast(box_height, tf.float32)\n",
    "    #Generate the new bbox vector for label\n",
    "    bbox = tf.concat(axis=0, values=[grid_id, center_x_percent, center_y_percent, box_width, box_height, label])\n",
    "    bbox = tf.transpose(bbox, [1, 0])\n",
    "\n",
    "    return image_train, parsed_features[\"filename\"], image_raw, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct the train dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/roy/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/roy/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1241: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_files = tf.data.Dataset.list_files(\"train_tf/*.tfrecord\")\n",
    "    dataset_train = train_files.interleave(tf.data.TFRecordDataset, cycle_length=4, num_parallel_calls=4)\n",
    "    dataset_train = dataset_train.shuffle(buffer_size=epoch_size)\n",
    "    dataset_train = dataset_train.repeat(100)\n",
    "    dataset_train = dataset_train.map(_parse_function, num_parallel_calls=12)\n",
    "    dataset_train = dataset_train.padded_batch(batch_size, \\\n",
    "                                               padded_shapes=([None,None,None], [], \\\n",
    "                                                              [None,None,None], [None,None]))\n",
    "    dataset_train = dataset_train.prefetch(batch_size)\n",
    "    iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n",
    "    image_train, filename, image_decoded, bbox = iterator.get_next()\n",
    "    train_init_op = iterator.make_initializer(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify the train data is correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op)\n",
    "    images_r, images_t_r, filename_r, bbox_run = sess.run([image_decoded, image_train, filename, bbox])\n",
    "    \n",
    "#Show the random crop and flip image with the bbox\n",
    "image_index = 9     #select one image in the batch\n",
    "image = images_t_r[image_index]\n",
    "image_bbox = bbox_run[image_index]\n",
    "\n",
    "for i in range(image_bbox.shape[0]):\n",
    "    if image_bbox[i][3]==0.0 or image_bbox[i][4]==0.0:\n",
    "        continue\n",
    "    else:\n",
    "        center_x = grid_width*image_bbox[i][1]+image_bbox[i][0]%grids*grid_width\n",
    "        center_y = grid_height*image_bbox[i][2]+image_bbox[i][0]//grids*grid_height\n",
    "        xmin = int(center_x - image_bbox[i][3]*image_width//2)\n",
    "        xmax = int(center_x + image_bbox[i][3]*image_width//2)\n",
    "        ymin = int(center_y - image_bbox[i][4]*image_height//2)\n",
    "        ymax = int(center_y + image_bbox[i][4]*image_height//2)\n",
    "        cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (0,255,0), 2)\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the IOU and LOSS calculation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the IOU, the merged is two rect vectors combined\n",
    "#Each rect vecor has 4 elements, xmin, xmax, ymin and ymax\n",
    "def calculate_IOU(merged):\n",
    "    rect1 = merged[:4]\n",
    "    rect2 = merged[4:8]\n",
    "    IOU = 0.0\n",
    "    IOU_area = 0.0\n",
    "    xmin=0.0\n",
    "    xmax=0.0\n",
    "    ymin=0.0\n",
    "    ymax= 0.0\n",
    "    rect1_area=0.0\n",
    "    rect2_area= 0.0\n",
    "    if (rect1[0]>=rect2[1] or rect2[0]>=rect1[1] or rect1[2]>=rect2[3] or rect2[2]>=rect1[3]):\n",
    "        IOU = 0.0\n",
    "    else:\n",
    "        xmin=tf.maximum(rect1[0], rect3[0])\n",
    "        xmax=tf.minimum(rect1[1], rect3[1])\n",
    "        ymin=tf.maximum(rect1[2], rect3[2])\n",
    "        ymax=tf.minimum(rect1[3], rect3[3])\n",
    "        IOU_area=(xmax-xmin)*(ymax-ymin)\n",
    "        rect1_area=(rect1[1]-rect1[0])*(rect1[3]-rect1[2])\n",
    "        rect3_area=(rect3[1]-rect3[0])*(rect3[3]-rect3[2])\n",
    "        IOU=IOU_area / (rect1_area+rect3_area-IOU_area)\n",
    "    return IOU\n",
    "tf_calculate_IOU = autograph.to_graph(calculate_IOU)\n",
    "\n",
    "#Calculate the loss. Based on the YOLO V1 paper\n",
    "#The pred is the prediction vector with shape [batchsize*7*7,30].\n",
    "#The label is the vector with shape [batchsize*7*7,26].\n",
    "def loss_func(pred, label):\n",
    "    #Divide the pred and label vectors by the mask for with object or non object.\n",
    "    mask_obj = label[:,1]>0.0\n",
    "    mask_noobj = label[:,1]<1.0\n",
    "    pred_obj = tf.boolean_mask(pred, mask_obj)\n",
    "    label_obj = tf.boolean_mask(label, mask_obj)\n",
    "    pred_noobj = tf.boolean_mask(pred, mask_noobj)\n",
    "    label_noobj = tf.boolean_mask(label, mask_noobj)\n",
    "    #Calculate the no obj prediction error\n",
    "    loss_noobj = tf.reduce_sum(tf.square(pred_noobj[:,0])+tf.square(pred_noobj[:,5]))\n",
    "    loss_classes = tf.reduce_sum(tf.square(pred_obj[:,10:]-label_obj[:,6:]))\n",
    "    #Calculate the prediction box coordinates\n",
    "    center_x1 = pred_obj[:,1:2]*grid_width\n",
    "    center_y1 = pred_obj[:,2:3]*grid_height\n",
    "    xmin_1 = center_x1 - pred_obj[:,3:4]**2*image_width//2\n",
    "    xmax_1 = center_x1 + pred_obj[:,3:4]**2*image_width//2\n",
    "    ymin_1 = center_y1 - pred_obj[:,4:5]**2*image_height//2\n",
    "    ymax_1 = center_y1 + pred_obj[:,4:5]**2*image_height//2\n",
    "    center_x2 = pred_obj[:,6:7]*grid_width\n",
    "    center_y2 = pred_obj[:,7:8]*grid_height\n",
    "    xmin_2 = center_x2 - pred_obj[:,8:9]**2*image_width//2\n",
    "    xmax_2 = center_x2 + pred_obj[:,8:9]**2*image_width//2\n",
    "    ymin_2 = center_y2 - pred_obj[:,9:10]**2*image_height//2\n",
    "    ymax_2 = center_y2 + pred_obj[:,9:10]**2*image_height//2\n",
    "    #Calculate the label box coordinates\n",
    "    center_x = label_obj[:,2:3]*grid_width\n",
    "    center_y = label_obj[:,3:4]*grid_height\n",
    "    xmin = center_x - label_obj[:,4:5]*image_width//2\n",
    "    xmax = center_x + label_obj[:,4:5]*image_width//2\n",
    "    ymin = center_y - label_obj[:,5:6]*image_height//2\n",
    "    ymax = center_y + label_obj[:,5:6]*image_height//2\n",
    "    #Concat the prediction box and ground truth box and calculate the IOU\n",
    "    merged1 = tf.concat([xmin_1,xmax_1,ymin_1,ymax_1,xmin,xmax,ymin,ymax], -1)\n",
    "    merged2 = tf.concat([xmin_2,xmax_2,ymin_2,ymax_2,xmin,xmax,ymin,ymax], -1)\n",
    "    IOU1 = tf.map_fn(tf_calculate_IOU, merged1)\n",
    "    IOU2 = tf.map_fn(tf_calculate_IOU, merged2)\n",
    "    #Select the higher IOU prediction box for coordination loss calculation\n",
    "    IOU1_mask = tf.math.greater_equal(IOU1,IOU2)\n",
    "    IOU2_mask = tf.math.greater(IOU2,IOU1)\n",
    "    coord_IOU1 = tf.boolean_mask(pred_obj[:,:5], IOU1_mask)\n",
    "    label_IOU1 = tf.boolean_mask(label_obj[:,2:6], IOU1_mask)\n",
    "    coord_IOU2 = tf.boolean_mask(pred_obj[:,5:10], IOU2_mask)\n",
    "    label_IOU2 = tf.boolean_mask(label_obj[:,2:6], IOU2_mask)\n",
    "    loss_coord = lambda_obj * (tf.reduce_sum( \\\n",
    "                 tf.square(coord_IOU1[:,1]-label_IOU1[:,0]) + \\\n",
    "                 tf.square(coord_IOU1[:,2]-label_IOU1[:,1]) + \\\n",
    "                 tf.square(coord_IOU1[:,3]-tf.sqrt(label_IOU1[:,2])) + \\\n",
    "                 tf.square(coord_IOU1[:,4]-tf.sqrt(label_IOU1[:,3])))+ \\\n",
    "                 tf.reduce_sum( \\\n",
    "                 tf.square(coord_IOU2[:,1]-label_IOU2[:,0]) + \\\n",
    "                 tf.square(coord_IOU2[:,2]-label_IOU2[:,1]) + \\\n",
    "                 tf.square(coord_IOU2[:,3]-tf.sqrt(label_IOU2[:,2])) + \\\n",
    "                 tf.square(coord_IOU2[:,4]-tf.sqrt(label_IOU2[:,3]))))\n",
    "    #Calculate the confidence for these two prediction boxes\n",
    "    loss_confidence = tf.reduce_sum(tf.square(pred_obj[:,0]-IOU1)+tf.square(pred_obj[:,5]-IOU2))\n",
    "    #Sum up all the loss parts\n",
    "    loss = (loss_noobj+loss_classes+loss_coord+loss_confidence)/batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/roy/AI/pascal/yolonet_model.py:10: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /home/roy/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/roy/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model_yolo/model.ckpt-30000\n",
      "step: 30100, Learning_rate:0.001000, Time: 103s Loss: 1.224235\n",
      "step: 30200, Learning_rate:0.001000, Time: 96s Loss: 1.198330\n",
      "step: 30300, Learning_rate:0.001000, Time: 95s Loss: 1.204168\n",
      "step: 30400, Learning_rate:0.001000, Time: 96s Loss: 1.225659\n",
      "step: 30500, Learning_rate:0.001000, Time: 96s Loss: 1.218227\n",
      "step: 30600, Learning_rate:0.001000, Time: 96s Loss: 1.200745\n",
      "step: 30700, Learning_rate:0.001000, Time: 94s Loss: 1.185329\n",
      "step: 30800, Learning_rate:0.001000, Time: 95s Loss: 1.213746\n",
      "step: 30900, Learning_rate:0.000100, Time: 95s Loss: 1.177624\n",
      "step: 31000, Learning_rate:0.000100, Time: 96s Loss: 1.207570\n",
      "step: 31100, Learning_rate:0.000100, Time: 95s Loss: 1.193434\n",
      "step: 31200, Learning_rate:0.000100, Time: 95s Loss: 1.170741\n",
      "step: 31300, Learning_rate:0.000100, Time: 95s Loss: 1.189242\n",
      "step: 31400, Learning_rate:0.000100, Time: 95s Loss: 1.154905\n",
      "step: 31500, Learning_rate:0.000100, Time: 96s Loss: 1.191800\n",
      "step: 31600, Learning_rate:0.000100, Time: 95s Loss: 1.189312\n",
      "step: 31700, Learning_rate:0.000100, Time: 95s Loss: 1.222892\n",
      "step: 31800, Learning_rate:0.000100, Time: 95s Loss: 1.198150\n",
      "step: 31900, Learning_rate:0.000100, Time: 95s Loss: 1.208728\n",
      "step: 32000, Learning_rate:0.000100, Time: 94s Loss: 1.161504\n",
      "step: 32100, Learning_rate:0.000100, Time: 95s Loss: 1.166347\n",
      "step: 32200, Learning_rate:0.000100, Time: 94s Loss: 1.192299\n",
      "step: 32300, Learning_rate:0.000100, Time: 95s Loss: 1.235452\n",
      "step: 32400, Learning_rate:0.000100, Time: 94s Loss: 1.227055\n",
      "step: 32500, Learning_rate:0.000100, Time: 93s Loss: 1.156639\n",
      "step: 32600, Learning_rate:0.000100, Time: 94s Loss: 1.183528\n",
      "step: 32700, Learning_rate:0.000100, Time: 94s Loss: 1.244799\n",
      "step: 32800, Learning_rate:0.000100, Time: 95s Loss: 1.196423\n",
      "step: 32900, Learning_rate:0.000100, Time: 96s Loss: 1.207955\n",
      "step: 33000, Learning_rate:0.000100, Time: 95s Loss: 1.174382\n",
      "step: 33100, Learning_rate:0.000100, Time: 95s Loss: 1.189081\n",
      "step: 33200, Learning_rate:0.000100, Time: 95s Loss: 1.204441\n",
      "step: 33300, Learning_rate:0.000100, Time: 96s Loss: 1.228973\n",
      "step: 33400, Learning_rate:0.000100, Time: 96s Loss: 1.213767\n",
      "step: 33500, Learning_rate:0.000100, Time: 95s Loss: 1.182986\n",
      "step: 33600, Learning_rate:0.000100, Time: 98s Loss: 1.187005\n",
      "step: 33700, Learning_rate:0.000100, Time: 96s Loss: 1.187350\n",
      "step: 33800, Learning_rate:0.000100, Time: 95s Loss: 1.202190\n",
      "step: 33900, Learning_rate:0.000100, Time: 95s Loss: 1.170966\n",
      "step: 34000, Learning_rate:0.000100, Time: 96s Loss: 1.158488\n",
      "step: 34100, Learning_rate:0.000100, Time: 96s Loss: 1.222141\n",
      "step: 34200, Learning_rate:0.000100, Time: 95s Loss: 1.240008\n",
      "step: 34300, Learning_rate:0.000100, Time: 95s Loss: 1.183865\n",
      "step: 34400, Learning_rate:0.000100, Time: 95s Loss: 1.176392\n",
      "step: 34500, Learning_rate:0.000100, Time: 94s Loss: 1.163909\n",
      "step: 34600, Learning_rate:0.000100, Time: 94s Loss: 1.155089\n",
      "step: 34700, Learning_rate:0.000100, Time: 94s Loss: 1.179153\n",
      "step: 34800, Learning_rate:0.000100, Time: 94s Loss: 1.181811\n",
      "step: 34900, Learning_rate:0.000100, Time: 95s Loss: 1.224673\n",
      "step: 35000, Learning_rate:0.000100, Time: 94s Loss: 1.182825\n",
      "step: 35100, Learning_rate:0.000100, Time: 94s Loss: 1.197765\n",
      "step: 35200, Learning_rate:0.000100, Time: 94s Loss: 1.188920\n",
      "step: 35300, Learning_rate:0.000100, Time: 94s Loss: 1.207914\n",
      "step: 35400, Learning_rate:0.000100, Time: 94s Loss: 1.214942\n",
      "step: 35500, Learning_rate:0.000100, Time: 94s Loss: 1.179531\n",
      "step: 35600, Learning_rate:0.000100, Time: 94s Loss: 1.159717\n",
      "step: 35700, Learning_rate:0.000100, Time: 95s Loss: 1.210809\n",
      "step: 35800, Learning_rate:0.000100, Time: 94s Loss: 1.175033\n",
      "step: 35900, Learning_rate:0.000100, Time: 94s Loss: 1.192032\n",
      "step: 36000, Learning_rate:0.000100, Time: 94s Loss: 1.193585\n",
      "step: 36100, Learning_rate:0.000100, Time: 96s Loss: 1.203637\n",
      "step: 36200, Learning_rate:0.000100, Time: 94s Loss: 1.156742\n",
      "step: 36300, Learning_rate:0.000100, Time: 95s Loss: 1.194533\n",
      "step: 36400, Learning_rate:0.000100, Time: 94s Loss: 1.202812\n",
      "step: 36500, Learning_rate:0.000100, Time: 94s Loss: 1.194546\n",
      "step: 36600, Learning_rate:0.000100, Time: 94s Loss: 1.180238\n",
      "step: 36700, Learning_rate:0.000100, Time: 94s Loss: 1.192171\n",
      "step: 36800, Learning_rate:0.000100, Time: 94s Loss: 1.180230\n",
      "step: 36900, Learning_rate:0.000100, Time: 95s Loss: 1.218570\n",
      "step: 37000, Learning_rate:0.000100, Time: 94s Loss: 1.165562\n",
      "step: 37100, Learning_rate:0.000100, Time: 94s Loss: 1.174612\n",
      "step: 37200, Learning_rate:0.000100, Time: 94s Loss: 1.181611\n",
      "step: 37300, Learning_rate:0.000100, Time: 95s Loss: 1.197218\n",
      "step: 37400, Learning_rate:0.000100, Time: 94s Loss: 1.176480\n",
      "step: 37500, Learning_rate:0.000100, Time: 94s Loss: 1.182807\n",
      "step: 37600, Learning_rate:0.000100, Time: 94s Loss: 1.173942\n",
      "step: 37700, Learning_rate:0.000100, Time: 94s Loss: 1.215516\n",
      "step: 37800, Learning_rate:0.000100, Time: 94s Loss: 1.190661\n",
      "step: 37900, Learning_rate:0.000100, Time: 94s Loss: 1.168671\n",
      "step: 38000, Learning_rate:0.000100, Time: 95s Loss: 1.215166\n",
      "step: 38100, Learning_rate:0.000100, Time: 94s Loss: 1.173041\n",
      "step: 38200, Learning_rate:0.000100, Time: 95s Loss: 1.206484\n",
      "step: 38300, Learning_rate:0.000100, Time: 95s Loss: 1.197213\n",
      "step: 38400, Learning_rate:0.000100, Time: 94s Loss: 1.207759\n",
      "step: 38500, Learning_rate:0.000100, Time: 93s Loss: 1.141424\n",
      "step: 38600, Learning_rate:0.000100, Time: 94s Loss: 1.181639\n",
      "step: 38700, Learning_rate:0.000100, Time: 95s Loss: 1.205919\n",
      "step: 38800, Learning_rate:0.000100, Time: 94s Loss: 1.171285\n",
      "step: 38900, Learning_rate:0.000100, Time: 94s Loss: 1.171709\n",
      "step: 39000, Learning_rate:0.000100, Time: 94s Loss: 1.160256\n",
      "step: 39100, Learning_rate:0.000100, Time: 94s Loss: 1.174241\n",
      "step: 39200, Learning_rate:0.000100, Time: 94s Loss: 1.177928\n",
      "step: 39300, Learning_rate:0.000100, Time: 94s Loss: 1.167706\n",
      "step: 39400, Learning_rate:0.000100, Time: 95s Loss: 1.189020\n",
      "step: 39500, Learning_rate:0.000100, Time: 94s Loss: 1.174151\n",
      "step: 39600, Learning_rate:0.000100, Time: 94s Loss: 1.178275\n",
      "step: 39700, Learning_rate:0.000100, Time: 94s Loss: 1.188907\n",
      "step: 39800, Learning_rate:0.000100, Time: 94s Loss: 1.167503\n",
      "step: 39900, Learning_rate:0.000100, Time: 94s Loss: 1.159565\n",
      "step: 40000, Learning_rate:0.000100, Time: 95s Loss: 1.209985\n",
      "step: 40100, Learning_rate:0.000100, Time: 94s Loss: 1.158659\n",
      "step: 40200, Learning_rate:0.000100, Time: 94s Loss: 1.180493\n",
      "step: 40300, Learning_rate:0.000100, Time: 95s Loss: 1.206591\n",
      "step: 40400, Learning_rate:0.000100, Time: 94s Loss: 1.172260\n",
      "step: 40500, Learning_rate:0.000100, Time: 94s Loss: 1.156640\n",
      "step: 40600, Learning_rate:0.000100, Time: 94s Loss: 1.172605\n",
      "step: 40700, Learning_rate:0.000100, Time: 94s Loss: 1.197566\n",
      "step: 40800, Learning_rate:0.000100, Time: 94s Loss: 1.184886\n",
      "step: 40900, Learning_rate:0.000100, Time: 94s Loss: 1.191073\n",
      "step: 41000, Learning_rate:0.000100, Time: 94s Loss: 1.194743\n",
      "step: 41100, Learning_rate:0.000100, Time: 94s Loss: 1.168596\n",
      "step: 41200, Learning_rate:0.000100, Time: 94s Loss: 1.189141\n",
      "step: 41300, Learning_rate:0.000100, Time: 94s Loss: 1.166371\n",
      "step: 41400, Learning_rate:0.000100, Time: 94s Loss: 1.160086\n",
      "step: 41500, Learning_rate:0.000100, Time: 94s Loss: 1.195400\n",
      "step: 41600, Learning_rate:0.000100, Time: 95s Loss: 1.200987\n",
      "step: 41700, Learning_rate:0.000100, Time: 94s Loss: 1.162122\n",
      "step: 41800, Learning_rate:0.000100, Time: 94s Loss: 1.141572\n",
      "step: 41900, Learning_rate:0.000100, Time: 95s Loss: 1.207052\n",
      "step: 42000, Learning_rate:0.000100, Time: 95s Loss: 1.216646\n",
      "WARNING:tensorflow:From /home/roy/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "step: 42100, Learning_rate:0.000100, Time: 94s Loss: 1.151172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42200, Learning_rate:0.000100, Time: 94s Loss: 1.193948\n",
      "step: 42300, Learning_rate:0.000100, Time: 94s Loss: 1.178660\n",
      "step: 42400, Learning_rate:0.000100, Time: 95s Loss: 1.190530\n",
      "step: 42500, Learning_rate:0.000100, Time: 94s Loss: 1.194576\n",
      "step: 42600, Learning_rate:0.000100, Time: 94s Loss: 1.164123\n",
      "step: 42700, Learning_rate:0.000100, Time: 95s Loss: 1.188197\n",
      "step: 42800, Learning_rate:0.000100, Time: 94s Loss: 1.170009\n",
      "step: 42900, Learning_rate:0.000100, Time: 95s Loss: 1.180124\n",
      "step: 43000, Learning_rate:0.000100, Time: 94s Loss: 1.167312\n",
      "step: 43100, Learning_rate:0.000100, Time: 94s Loss: 1.194800\n",
      "step: 43200, Learning_rate:0.000100, Time: 94s Loss: 1.186283\n",
      "step: 43300, Learning_rate:0.000100, Time: 94s Loss: 1.171472\n",
      "step: 43400, Learning_rate:0.000100, Time: 95s Loss: 1.175364\n",
      "step: 43500, Learning_rate:0.000100, Time: 94s Loss: 1.163143\n",
      "step: 43600, Learning_rate:0.000100, Time: 94s Loss: 1.177204\n",
      "step: 43700, Learning_rate:0.000100, Time: 94s Loss: 1.157613\n",
      "step: 43800, Learning_rate:0.000100, Time: 95s Loss: 1.197381\n",
      "step: 43900, Learning_rate:0.000100, Time: 94s Loss: 1.168954\n",
      "step: 44000, Learning_rate:0.000100, Time: 94s Loss: 1.175572\n",
      "step: 44100, Learning_rate:0.000100, Time: 95s Loss: 1.201701\n",
      "step: 44200, Learning_rate:0.000100, Time: 94s Loss: 1.167059\n",
      "step: 44300, Learning_rate:0.000100, Time: 93s Loss: 1.134195\n",
      "step: 44400, Learning_rate:0.000100, Time: 95s Loss: 1.204325\n",
      "step: 44500, Learning_rate:0.000100, Time: 95s Loss: 1.176756\n",
      "step: 44600, Learning_rate:0.000100, Time: 94s Loss: 1.186443\n",
      "step: 44700, Learning_rate:0.000100, Time: 95s Loss: 1.180921\n",
      "step: 44800, Learning_rate:0.000100, Time: 94s Loss: 1.167881\n",
      "step: 44900, Learning_rate:0.000100, Time: 94s Loss: 1.147312\n",
      "step: 45000, Learning_rate:0.000100, Time: 95s Loss: 1.178523\n",
      "step: 45100, Learning_rate:0.000100, Time: 94s Loss: 1.171769\n",
      "step: 45200, Learning_rate:0.000100, Time: 95s Loss: 1.194363\n",
      "step: 45300, Learning_rate:0.000100, Time: 95s Loss: 1.190746\n",
      "step: 45400, Learning_rate:0.000100, Time: 94s Loss: 1.161737\n",
      "step: 45500, Learning_rate:0.000100, Time: 94s Loss: 1.154595\n",
      "step: 45600, Learning_rate:0.000100, Time: 95s Loss: 1.193179\n",
      "step: 45700, Learning_rate:0.000100, Time: 94s Loss: 1.161266\n",
      "step: 45800, Learning_rate:0.000100, Time: 95s Loss: 1.188512\n",
      "step: 45900, Learning_rate:0.000100, Time: 94s Loss: 1.162686\n",
      "step: 46000, Learning_rate:0.000100, Time: 94s Loss: 1.163662\n",
      "step: 46100, Learning_rate:0.000100, Time: 95s Loss: 1.160839\n",
      "step: 46200, Learning_rate:0.000100, Time: 93s Loss: 1.108105\n",
      "step: 46300, Learning_rate:0.000100, Time: 95s Loss: 1.193135\n",
      "step: 46400, Learning_rate:0.000100, Time: 95s Loss: 1.198232\n",
      "step: 46500, Learning_rate:0.000100, Time: 95s Loss: 1.175693\n",
      "step: 46600, Learning_rate:0.000100, Time: 94s Loss: 1.169338\n",
      "step: 46700, Learning_rate:0.000100, Time: 95s Loss: 1.204929\n",
      "step: 46800, Learning_rate:0.000100, Time: 94s Loss: 1.179578\n",
      "step: 46900, Learning_rate:0.000100, Time: 94s Loss: 1.172357\n",
      "step: 47000, Learning_rate:0.000100, Time: 94s Loss: 1.166595\n",
      "step: 47100, Learning_rate:0.000100, Time: 94s Loss: 1.176135\n",
      "step: 47200, Learning_rate:0.000100, Time: 94s Loss: 1.161713\n",
      "step: 47300, Learning_rate:0.000100, Time: 94s Loss: 1.153292\n",
      "step: 47400, Learning_rate:0.000100, Time: 94s Loss: 1.151942\n",
      "step: 47500, Learning_rate:0.000100, Time: 95s Loss: 1.202589\n",
      "step: 47600, Learning_rate:0.000100, Time: 94s Loss: 1.166631\n",
      "step: 47700, Learning_rate:0.000100, Time: 94s Loss: 1.175498\n",
      "step: 47800, Learning_rate:0.000100, Time: 95s Loss: 1.182783\n",
      "step: 47900, Learning_rate:0.000100, Time: 94s Loss: 1.171614\n",
      "step: 48000, Learning_rate:0.000100, Time: 95s Loss: 1.179411\n",
      "step: 48100, Learning_rate:0.000100, Time: 95s Loss: 1.173297\n",
      "step: 48200, Learning_rate:0.000100, Time: 94s Loss: 1.172398\n",
      "step: 48300, Learning_rate:0.000100, Time: 94s Loss: 1.172208\n",
      "step: 48400, Learning_rate:0.000100, Time: 94s Loss: 1.150490\n",
      "step: 48500, Learning_rate:0.000100, Time: 95s Loss: 1.182087\n",
      "step: 48600, Learning_rate:0.000100, Time: 94s Loss: 1.187767\n",
      "step: 48700, Learning_rate:0.000100, Time: 95s Loss: 1.189658\n",
      "step: 48800, Learning_rate:0.000100, Time: 94s Loss: 1.169347\n",
      "step: 48900, Learning_rate:0.000100, Time: 93s Loss: 1.139819\n",
      "step: 49000, Learning_rate:0.000100, Time: 94s Loss: 1.151100\n",
      "step: 49100, Learning_rate:0.000100, Time: 95s Loss: 1.168601\n",
      "step: 49200, Learning_rate:0.000100, Time: 96s Loss: 1.193620\n",
      "step: 49300, Learning_rate:0.000100, Time: 96s Loss: 1.175264\n",
      "step: 49400, Learning_rate:0.000100, Time: 96s Loss: 1.184669\n",
      "step: 49500, Learning_rate:0.000100, Time: 96s Loss: 1.171262\n",
      "step: 49600, Learning_rate:0.000100, Time: 97s Loss: 1.159506\n",
      "step: 49700, Learning_rate:0.000100, Time: 96s Loss: 1.217952\n",
      "step: 49800, Learning_rate:0.000100, Time: 96s Loss: 1.162301\n",
      "step: 49900, Learning_rate:0.000100, Time: 97s Loss: 1.174854\n",
      "step: 50000, Learning_rate:0.000100, Time: 97s Loss: 1.162578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fca008b5af44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mgrids_vector_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrids_vector_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mloss_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_train_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimages_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrids_vector_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgrids_vector_run\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_train_batch = tf.placeholder(shape=[None, image_height, image_width, 3], dtype=tf.float32)\n",
    "grids_vector_batch = tf.placeholder(shape=[None, grids*grids, 26], dtype=tf.float32)\n",
    "result = yolonet_model.inference(image_train_batch, pretrain_trainable=False, wd=0.0005, pretrain_training=False, yolo_training=True)\n",
    "result = tf.reshape(result, [-1,30])\n",
    "grids_vector = tf.reshape(grids_vector_batch, [-1,26])\n",
    "mse_loss = loss_func(result, grids_vector)\n",
    "tf.add_to_collection('losses', mse_loss)\n",
    "loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "epoch_steps = int(epoch_size/batch_size)\n",
    "boundaries = [epoch_steps*5,epoch_steps*55,epoch_steps*75]\n",
    "values = [0.001, 0.01, 0.001, 0.0001]\n",
    "learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimize_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "#Load the pretrain Imagenet weights\n",
    "#For the first time training, cancel the comment of below codes.\n",
    "'''\n",
    "variables_list = []\n",
    "for var in tf.all_variables():\n",
    "    #The var with \"new\" means it's for object dection training, not included in Imagenet pretrain\n",
    "    if 'new' in var.name or 'Variable' in var.name:\n",
    "        continue\n",
    "    else:\n",
    "        variables_list.append(var)\n",
    "saver=tf.train.Saver(variables_list)\n",
    "'''\n",
    "saver_yolo=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #For first time training, cancel the comment of below codes.\n",
    "    '''\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Load the pretrained Imagenet weights\n",
    "    saver.restore(sess, \"/home/roy/AI/model_bn_loss/model.ckpt-105000\")   \n",
    "    sess.run(global_step.initializer)\n",
    "    '''\n",
    "    #Comment out below line if the first training\n",
    "    saver_yolo.restore(sess, \"model_yolo/model.ckpt-30000\")   \n",
    "    sess.run([train_init_op])\n",
    "    total_loss = 0.0 \n",
    "    starttime = time.time()\n",
    "    while(True):\n",
    "        try:\n",
    "            images_run, bbox_run = sess.run([image_train, bbox])\n",
    "            #Construct the grids_vector based on bbox_run\n",
    "            batch_num, box_num, _ = bbox_run.shape\n",
    "            grids_vector_list = []\n",
    "            for i in range(batch_num):\n",
    "                vector = np.zeros([grids*grids, 26], dtype=float)\n",
    "                for j in range(box_num):\n",
    "                    if bbox_run[i][j][3]==0.0 or bbox_run[i][j][4]==0.0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        grid_id = int(bbox_run[i][j][0])\n",
    "                        vector[grid_id][0] = grid_id\n",
    "                        vector[grid_id][1] = 1.0\n",
    "                        vector[grid_id][2] = bbox_run[i][j][1]\n",
    "                        vector[grid_id][3] = bbox_run[i][j][2]\n",
    "                        vector[grid_id][4] = bbox_run[i][j][3]\n",
    "                        vector[grid_id][5] = bbox_run[i][j][4]\n",
    "                        label = int(bbox_run[i][j][5])\n",
    "                        vector[grid_id][label+6] = 1.0\n",
    "                grids_vector_list.append(vector)\n",
    "            grids_vector_run = np.stack(grids_vector_list)\n",
    "            \n",
    "            loss_a, step, lr, _ = sess.run([loss, global_step, learning_rate, optimize_op], feed_dict={image_train_batch:images_run, grids_vector_batch:grids_vector_run})\n",
    "            total_loss += loss_a\n",
    "        \n",
    "            if step%100==0:\n",
    "                print(\"step: %i, Learning_rate:%f, Time: %is Loss: %f\" \\\n",
    "                      %(step, lr, int(time.time()-starttime), total_loss/100))\n",
    "                total_loss = 0.0\n",
    "                starttime = time.time()\n",
    "    \n",
    "            if step%2000==0:\n",
    "                save_path = saver_yolo.save(sess, \"model_yolo/model.ckpt\", global_step=global_step)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
